# 🧠 CSE552 – Machine Learning Homework Series (Spring 2025)

> 🎓 Gebze Technical University – Graduate Level  
> 🗂️ Homework 1–5 Full Implementations  
> 🧾 PDF reports, Jupyter Notebooks, custom functions & visualizations  
> 🔗 Dataset sources: UCI ML Repository, MNIST (Keras)

---

## 🌟 Project Overview | Proje Genel Bakış

**📘 English:**  
This repository contains the complete implementation and analysis of **five major homework projects** assigned in CSE552 - Machine Learning course. The assignments span key topics from supervised learning (KNN, SVM, DT), feature reduction (PCA), unsupervised clustering (K-Means), to ensemble learning (AdaBoost + Perceptron Forest). Each task is implemented from scratch (unless otherwise specified), rigorously evaluated, and visually reported.

**📗 Türkçe:**  
Bu depo, CSE552 - Makine Öğrenmesi dersi kapsamında verilen **beş temel ödevi** içermektedir. Ödevler; denetimli öğrenme (KNN, SVM, Karar Ağaçları), boyut indirgeme (PCA), denetimsiz öğrenme (K-means Kümeleme) ve topluluk yöntemleri (AdaBoost + Perceptron Ormanı) gibi temel konuları kapsamaktadır. Her ödev, özenle kodlanmış, çapraz doğrulama ile test edilmiş ve grafiklerle desteklenmiştir.

---

## 📂 Homework Breakdown | Ödevlere Genel Bakış

### 📝 HW1 – Classification & Regression using KNN, SVM, DT
- ✅ Breast Cancer & Bike Sharing datasets
- ⚙️ KNN classifier (Euclidean), KNN regressor (Manhattan)
- 🔍 Linear SVM & Decision Tree with rule extraction
- 📊 Evaluation: Cross-validation, ROC, runtime comparisons

### 🌳 HW2 – Custom Decision Tree & Random Decision Forest
- 📂 Dataset: Abalone (UCI)
- ⚙️ Custom decision tree from scratch with numeric & categorical handling
- ✂️ Optional pruning support
- 🌲 Random Forest implemented manually
- 📈 Confusion matrix + cross-validation reporting

### 🧬 HW3 – Principal Component Analysis (PCA)
- 🖼️ Dataset: MNIST Digits (Keras)
- 🧠 Manual PCA implementation with SVD
- 🎨 2D/3D visualization using principal components
- 🌲 Classification with Random Forest on reduced features

### 🔗 HW4 – K-Means Clustering (L1, L2, Cosine)
- 🧮 Dataset: MNIST Digits
- 📌 Clustering with three distance metrics: Euclidean, Manhattan, Cosine
- 🧾 Custom contingency table & greedy label assignment
- 🔁 5-fold cross-validation with cluster-based evaluation

### 🔁 HW5 – Boosted MLP & Perceptron Forest
- 🧠 Dataset: Mushroom / Bank Marketing / Wine Quality (varies)
- 🤖 AdaBoost with MLP as weak learners
- 🌲 Perceptron-Node Based Decision Forest
- 📉 Metrics: Accuracy, F1, AUC-ROC, training time
- 📚 Model comparison with visualizations & cross-validation

---

## 📌 Highlights | Öne Çıkanlar
- 📊 **All results include precision, recall, F1-score, AUC, confusion matrices**
- 📚 **PDF reports included** for each homework
- 🔍 **Hand-coded implementations** wherever required (e.g., PCA, KNN, DT)
- 🎨 **Custom graphs and interpretation included** (matplotlib, seaborn)

---

## 💻 Run the Notebooks | Notebookları Çalıştırmak
Clone the repo and open the `.ipynb` files on **Google Colab** or **Jupyter Notebook**:
