# ğŸ§  CSE552 â€“ Machine Learning Homework Series (Spring 2025)

> ğŸ“ Gebze Technical University â€“ Graduate Level  
> ğŸ—‚ï¸ Homework 1â€“5 Full Implementations  
> ğŸ§¾ PDF reports, Jupyter Notebooks, custom functions & visualizations  
> ğŸ”— Dataset sources: UCI ML Repository, MNIST (Keras)

---

## ğŸŒŸ Project Overview | Proje Genel BakÄ±ÅŸ

**ğŸ“˜ English:**  
This repository contains the complete implementation and analysis of **five major homework projects** assigned in CSE552 - Machine Learning course. The assignments span key topics from supervised learning (KNN, SVM, DT), feature reduction (PCA), unsupervised clustering (K-Means), to ensemble learning (AdaBoost + Perceptron Forest). Each task is implemented from scratch (unless otherwise specified), rigorously evaluated, and visually reported.

**ğŸ“— TÃ¼rkÃ§e:**  
Bu depo, CSE552 - Makine Ã–ÄŸrenmesi dersi kapsamÄ±nda verilen **beÅŸ temel Ã¶devi** iÃ§ermektedir. Ã–devler; denetimli Ã¶ÄŸrenme (KNN, SVM, Karar AÄŸaÃ§larÄ±), boyut indirgeme (PCA), denetimsiz Ã¶ÄŸrenme (K-means KÃ¼meleme) ve topluluk yÃ¶ntemleri (AdaBoost + Perceptron OrmanÄ±) gibi temel konularÄ± kapsamaktadÄ±r. Her Ã¶dev, Ã¶zenle kodlanmÄ±ÅŸ, Ã§apraz doÄŸrulama ile test edilmiÅŸ ve grafiklerle desteklenmiÅŸtir.

---

## ğŸ“‚ Homework Breakdown | Ã–devlere Genel BakÄ±ÅŸ

### ğŸ“ HW1 â€“ Classification & Regression using KNN, SVM, DT
- âœ… Breast Cancer & Bike Sharing datasets
- âš™ï¸ KNN classifier (Euclidean), KNN regressor (Manhattan)
- ğŸ” Linear SVM & Decision Tree with rule extraction
- ğŸ“Š Evaluation: Cross-validation, ROC, runtime comparisons

### ğŸŒ³ HW2 â€“ Custom Decision Tree & Random Decision Forest
- ğŸ“‚ Dataset: Abalone (UCI)
- âš™ï¸ Custom decision tree from scratch with numeric & categorical handling
- âœ‚ï¸ Optional pruning support
- ğŸŒ² Random Forest implemented manually
- ğŸ“ˆ Confusion matrix + cross-validation reporting

### ğŸ§¬ HW3 â€“ Principal Component Analysis (PCA)
- ğŸ–¼ï¸ Dataset: MNIST Digits (Keras)
- ğŸ§  Manual PCA implementation with SVD
- ğŸ¨ 2D/3D visualization using principal components
- ğŸŒ² Classification with Random Forest on reduced features

### ğŸ”— HW4 â€“ K-Means Clustering (L1, L2, Cosine)
- ğŸ§® Dataset: MNIST Digits
- ğŸ“Œ Clustering with three distance metrics: Euclidean, Manhattan, Cosine
- ğŸ§¾ Custom contingency table & greedy label assignment
- ğŸ” 5-fold cross-validation with cluster-based evaluation

### ğŸ” HW5 â€“ Boosted MLP & Perceptron Forest
- ğŸ§  Dataset: Mushroom / Bank Marketing / Wine Quality (varies)
- ğŸ¤– AdaBoost with MLP as weak learners
- ğŸŒ² Perceptron-Node Based Decision Forest
- ğŸ“‰ Metrics: Accuracy, F1, AUC-ROC, training time
- ğŸ“š Model comparison with visualizations & cross-validation

---

## ğŸ“Œ Highlights | Ã–ne Ã‡Ä±kanlar
- ğŸ“Š **All results include precision, recall, F1-score, AUC, confusion matrices**
- ğŸ“š **PDF reports included** for each homework
- ğŸ” **Hand-coded implementations** wherever required (e.g., PCA, KNN, DT)
- ğŸ¨ **Custom graphs and interpretation included** (matplotlib, seaborn)

---

## ğŸ’» Run the Notebooks | NotebooklarÄ± Ã‡alÄ±ÅŸtÄ±rmak
Clone the repo and open the `.ipynb` files on **Google Colab** or **Jupyter Notebook**:
