{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pAav0QW-6Ta7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import urllib.request\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Breast Cancer Wisconsin Diagnostic Dataset from UCI repository\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
        "data = urllib.request.urlopen(url).read().decode('utf-8')\n",
        "df_wdbc = pd.read_csv(io.StringIO(data), header=None)\n",
        "\n",
        "# Define column names\n",
        "columns = [\"ID\", \"Diagnosis\"]\n",
        "for i in range(1, 31):\n",
        "    columns.append(f\"Feature_{i}\")\n",
        "df_wdbc.columns = columns\n",
        "\n",
        "# Drop ID column and convert Diagnosis to binary\n",
        "df_wdbc.drop(\"ID\", axis=1, inplace=True)\n",
        "df_wdbc[\"Diagnosis\"] = df_wdbc[\"Diagnosis\"].map({\"M\": 1, \"B\": 0})\n",
        "\n",
        "# Separate features and target\n",
        "X_wdbc = df_wdbc.drop(\"Diagnosis\", axis=1).values  # Shape (569, 30)\n",
        "y_wdbc = df_wdbc[\"Diagnosis\"].values  # Shape (569, )\n",
        "\n",
        "print(\"WDBC Dataset shape:\", X_wdbc.shape, y_wdbc.shape)\n",
        "print(\"Class distribution (M=1, B=0):\")\n",
        "print(pd.Series(y_wdbc).value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqNM-nm06csl",
        "outputId": "f3c68104-0084-4aa2-e943-04f92525e5a3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WDBC Dataset shape: (569, 30) (569,)\n",
            "Class distribution (M=1, B=0):\n",
            "0    357\n",
            "1    212\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_distance(x1, x2):\n",
        "    \"\"\"\n",
        "    Calculate Euclidean distance between two points (NumPy arrays).\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    x1 : numpy.ndarray\n",
        "        First point\n",
        "    x2 : numpy.ndarray\n",
        "        Second point\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float: Euclidean distance between x1 and x2\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.sum((x1 - x2) ** 2))"
      ],
      "metadata": {
        "id": "nBs7UB1NrQ9e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def knn_classify(X_train, y_train, x_test, k=3):\n",
        "    \"\"\"\n",
        "    Perform K-Nearest Neighbors classification for a single test sample.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train : numpy.ndarray, shape (N, d)\n",
        "        Training feature matrix\n",
        "    y_train : numpy.ndarray, shape (N,)\n",
        "        Training labels (0 or 1)\n",
        "    x_test : numpy.ndarray, shape (d,)\n",
        "        Test sample to classify\n",
        "    k : int, optional (default=3)\n",
        "        Number of nearest neighbors to consider\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    int: Predicted label (0 or 1)\n",
        "    \"\"\"\n",
        "    # Calculate distances to all training points\n",
        "    distances = []\n",
        "    for i in range(len(X_train)):\n",
        "        dist = euclidean_distance(X_train[i], x_test)\n",
        "        distances.append((dist, y_train[i]))\n",
        "\n",
        "    # Sort distances\n",
        "    distances.sort(key=lambda x: x[0])\n",
        "\n",
        "    # Select k nearest neighbors\n",
        "    neighbors = distances[:k]\n",
        "\n",
        "    # Predict based on majority vote\n",
        "    labels = [n[1] for n in neighbors]\n",
        "    prediction = max(set(labels), key=labels.count)\n",
        "\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "6LmGBvWjw_T0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validation_knn_classification(X, y, k=3, n_folds=6):\n",
        "    \"\"\"\n",
        "    Perform k-fold cross-validation using K-Nearest Neighbors classifier.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : numpy.ndarray, shape (N, d)\n",
        "        Feature matrix\n",
        "    y : numpy.ndarray, shape (N,)\n",
        "        Target labels (0 or 1)\n",
        "    k : int, optional (default=3)\n",
        "        Number of neighbors for KNN\n",
        "    n_folds : int, optional (default=6)\n",
        "        Number of cross-validation folds\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple:\n",
        "        - Average accuracy (float)\n",
        "        - Confusion matrices for each fold (list)\n",
        "        - Average runtime (float)\n",
        "    \"\"\"\n",
        "    # Prepare for cross-validation\n",
        "    N = len(X)\n",
        "    fold_size = N // n_folds\n",
        "\n",
        "    # Shuffle indices\n",
        "    indices = np.arange(N)\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    # Metrics storage\n",
        "    accuracies = []\n",
        "    all_conf_matrices = []\n",
        "    fold_times = []\n",
        "\n",
        "    # Cross-validation loop\n",
        "    for fold_idx in range(n_folds):\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Define test and training indices\n",
        "        test_start = fold_idx * fold_size\n",
        "        test_end = test_start + fold_size\n",
        "        test_indices = indices[test_start:test_end]\n",
        "        train_indices = np.concatenate((indices[:test_start], indices[test_end:]))\n",
        "\n",
        "        # Split data\n",
        "        X_train, y_train = X[train_indices], y[train_indices]\n",
        "        X_test, y_test = X[test_indices], y[test_indices]\n",
        "\n",
        "        # Predict for each test sample\n",
        "        y_pred = []\n",
        "        for x_test in X_test:\n",
        "            pred = knn_classify(X_train, y_train, x_test, k=k)\n",
        "            y_pred.append(pred)\n",
        "        y_pred = np.array(y_pred)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        acc = np.mean(y_pred == y_test)\n",
        "        accuracies.append(acc)\n",
        "\n",
        "        # Compute confusion matrix\n",
        "        cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
        "        all_conf_matrices.append(cm)\n",
        "\n",
        "        # Record runtime\n",
        "        end_time = time.time()\n",
        "        fold_times.append(end_time - start_time)\n",
        "\n",
        "    # Compute average metrics\n",
        "    avg_accuracy = np.mean(accuracies)\n",
        "    avg_time = np.mean(fold_times)\n",
        "\n",
        "    return avg_accuracy, all_conf_matrices, avg_time"
      ],
      "metadata": {
        "id": "d-YUQAnuxGfx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment configuration\n",
        "k_value = 3\n",
        "n_folds = 6\n",
        "\n",
        "# Perform cross-validation\n",
        "avg_acc, conf_mats, runtime = cross_validation_knn_classification(\n",
        "    X_wdbc, y_wdbc, k=k_value, n_folds=n_folds\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(\"==== KNN Classification (WDBC Dataset) ====\")\n",
        "print(f\"K = {k_value}, Number of Folds = {n_folds}\")\n",
        "print(f\"Average Accuracy: {avg_acc:.4f}\")\n",
        "print(f\"Average Runtime (seconds): {runtime:.4f}\")\n",
        "\n",
        "# Print confusion matrices for each fold\n",
        "for i, cm in enumerate(conf_mats, 1):\n",
        "    print(f\"\\nConfusion Matrix for Fold {i} (row=actual, col=predicted) [B(0), M(1)]:\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QbVPjSzxHxX",
        "outputId": "ec23e96e-9909-493f-818e-1fa8cef60da2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== KNN Classification (WDBC Dataset) ====\n",
            "K = 3, Number of Folds = 6\n",
            "Average Accuracy: 0.9326\n",
            "Average Runtime (seconds): 1.5297\n",
            "\n",
            "Confusion Matrix for Fold 1 (row=actual, col=predicted) [B(0), M(1)]:\n",
            " [[55  2]\n",
            " [ 1 36]]\n",
            "\n",
            "Confusion Matrix for Fold 2 (row=actual, col=predicted) [B(0), M(1)]:\n",
            " [[60  2]\n",
            " [ 4 28]]\n",
            "\n",
            "Confusion Matrix for Fold 3 (row=actual, col=predicted) [B(0), M(1)]:\n",
            " [[55  1]\n",
            " [ 8 30]]\n",
            "\n",
            "Confusion Matrix for Fold 4 (row=actual, col=predicted) [B(0), M(1)]:\n",
            " [[55  4]\n",
            " [ 4 31]]\n",
            "\n",
            "Confusion Matrix for Fold 5 (row=actual, col=predicted) [B(0), M(1)]:\n",
            " [[57  3]\n",
            " [ 1 33]]\n",
            "\n",
            "Confusion Matrix for Fold 6 (row=actual, col=predicted) [B(0), M(1)]:\n",
            " [[57  2]\n",
            " [ 6 29]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Data Preparation\n",
        "\n",
        "  - Directly downloaded from UCI Machine Learning Repository\n",
        "  - 569 total samples, 30 feature dimensions, binary classsification\n",
        "  - Removed ID column(non-predictive feature)\n",
        "  - Converted categorical diagnosis to binary:\n",
        "    - Malignant(M) -> 1\n",
        "    - Benign(B) -> 0\n",
        "  - Seperated features (X) and target variable (y)\n",
        "\n",
        "---\n",
        "\n",
        "#2. Distance Metric Selection: Euclidean Distance\n",
        "\n",
        "  - Measures straight-line distance between data points\n",
        "  - Captures multi-ddimensional feature interactions\n",
        "  - Simple yet effective for medical diagnostic data\n",
        "\n",
        "---\n",
        "\n",
        "#3. KNN Classification Mechanism\n",
        "\n",
        "  - Calculate distances to all training points\n",
        "  - Sort distances in ascending order\n",
        "  - Select k-nearest neighbors (k=3 in this case)\n",
        "\n",
        "---\n",
        "\n",
        "#4. Cross-Validation Strategy\n",
        "\n",
        "  - 6-fold cross-validation\n",
        "  - Randommly shuffle and split data\n",
        "  - Ensures robust performance estimation\n",
        "  - Prevents overfitting\n",
        "  - provides comprehensive model evaluation\n",
        "\n",
        "---\n",
        "#Summary\n",
        "Our machine learning approach for breast cancer diagnosis achieved remarkable results through the K-Nearest Neighbors (**KNN**) algorithm. By meticulously preprocessing the data and evaluating 30 distinct features using Euclidean distance, we systematically identified similarities between data points. Through a rigorous 6-fold cross-validation process, we comprehensively tested the model's performance, achieving an impressive 93.26% accuracy. The low misclassification rates and consistent outcomes demonstrated the significant potential of this simple yet powerful algorithm in providing diagnostic support, showcasing how intelligent computational methods can effectively augment medical decision-making."
      ],
      "metadata": {
        "id": "k_XXL7ByJ1TJ"
      }
    }
  ]
}